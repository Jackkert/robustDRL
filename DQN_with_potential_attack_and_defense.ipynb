{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8bbf61",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800201a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "EPISODES = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "MEM_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.999\n",
    "EXPLORATION_MIN = 0.001\n",
    "EPSILON_PERT = 0.015\n",
    "\n",
    "FC1_DIMS = 1024\n",
    "FC2_DIMS = 512\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e224197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = env.observation_space.shape\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_shape, FC1_DIMS)\n",
    "        self.fc2 = nn.Linear(FC1_DIMS, FC2_DIMS)\n",
    "        self.fc3 = nn.Linear(FC2_DIMS, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.mem_count = 0\n",
    "\n",
    "        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
    "        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n",
    "\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        mem_index = self.mem_count % MEM_SIZE\n",
    "\n",
    "        self.states[mem_index]  = state.clone().detach().numpy()\n",
    "        self.actions[mem_index] = action\n",
    "        self.rewards[mem_index] = reward\n",
    "        self.states_[mem_index] = state_.clone().detach().numpy()\n",
    "        self.dones[mem_index] =  1 - done\n",
    "\n",
    "        self.mem_count += 1\n",
    "\n",
    "    def sample(self):\n",
    "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
    "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
    "\n",
    "        states  = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.states_[batch_indices]\n",
    "        dones   = self.dones[batch_indices]\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "class DQN_Solver:\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        state = observation\n",
    "        state = state.to(DEVICE)\n",
    "        state = state.unsqueeze(0)\n",
    "\n",
    "        q_values = self.network(state.float())\n",
    "        return torch.argmax(q_values).item(), q_values\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_count < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.memory.sample()\n",
    "        states = torch.tensor(states , dtype=torch.float32).to(DEVICE)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(DEVICE)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
    "        states_ = torch.tensor(states_, dtype=torch.float32).to(DEVICE)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(DEVICE)\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "        states.requires_grad = True\n",
    "        \n",
    "        q_values = self.network(states)\n",
    "        next_q_values = self.network(states_)\n",
    "\n",
    "        predicted_value_of_now = q_values[batch_indices, actions]\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "\n",
    "        q_target = rewards + GAMMA * predicted_value_of_future * dones\n",
    "\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)        \n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a0f3cc",
   "metadata": {},
   "source": [
    "## Training the network without defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a852a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12796/588032168.py:29: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Average Reward 30.0 Best Reward 30.0 Last Reward 30.0 Epsilon 1.0\n",
      "Episode 2 Average Reward 27.0 Best Reward 30.0 Last Reward 24.0 Epsilon 1.0\n",
      "Episode 3 Average Reward 29.666666666666668 Best Reward 35.0 Last Reward 35.0 Epsilon 0.9743224148844496\n",
      "Episode 4 Average Reward 33.25 Best Reward 44.0 Last Reward 44.0 Epsilon 0.9323611649219127\n",
      "Episode 5 Average Reward 29.6 Best Reward 44.0 Last Reward 15.0 Epsilon 0.9184732224159486\n",
      "Episode 6 Average Reward 30.0 Best Reward 44.0 Last Reward 32.0 Epsilon 0.8895331192339416\n",
      "Episode 7 Average Reward 31.0 Best Reward 44.0 Last Reward 37.0 Epsilon 0.857205969570888\n",
      "Episode 8 Average Reward 35.875 Best Reward 70.0 Last Reward 70.0 Epsilon 0.7992255563671304\n",
      "Episode 9 Average Reward 34.0 Best Reward 70.0 Last Reward 19.0 Epsilon 0.784176167005256\n",
      "Episode 10 Average Reward 32.5 Best Reward 70.0 Last Reward 19.0 Epsilon 0.7694101571203781\n",
      "Episode 11 Average Reward 32.0 Best Reward 70.0 Last Reward 27.0 Epsilon 0.7489039087598284\n",
      "Episode 12 Average Reward 30.666666666666668 Best Reward 70.0 Last Reward 16.0 Epsilon 0.737010896662273\n",
      "Episode 13 Average Reward 29.153846153846153 Best Reward 70.0 Last Reward 11.0 Epsilon 0.7289441910343799\n",
      "Episode 14 Average Reward 29.285714285714285 Best Reward 70.0 Last Reward 31.0 Epsilon 0.7066826263699144\n",
      "Episode 15 Average Reward 29.6 Best Reward 70.0 Last Reward 34.0 Epsilon 0.6830476698153162\n",
      "Episode 16 Average Reward 28.6875 Best Reward 70.0 Last Reward 15.0 Epsilon 0.6728733649170395\n",
      "Episode 17 Average Reward 28.11764705882353 Best Reward 70.0 Last Reward 19.0 Epsilon 0.660203182914977\n",
      "Episode 18 Average Reward 27.5 Best Reward 70.0 Last Reward 17.0 Epsilon 0.649069069067341\n",
      "Episode 19 Average Reward 27.31578947368421 Best Reward 70.0 Last Reward 24.0 Epsilon 0.6336692476264985\n",
      "Episode 20 Average Reward 28.55 Best Reward 70.0 Last Reward 52.0 Epsilon 0.6015448579979431\n",
      "Episode 21 Average Reward 27.761904761904763 Best Reward 70.0 Last Reward 12.0 Epsilon 0.5943658896200158\n",
      "Episode 22 Average Reward 27.318181818181817 Best Reward 70.0 Last Reward 18.0 Epsilon 0.5837577583990794\n",
      "Episode 23 Average Reward 28.956521739130434 Best Reward 70.0 Last Reward 65.0 Epsilon 0.5470026121551156\n",
      "Episode 24 Average Reward 29.166666666666668 Best Reward 70.0 Last Reward 34.0 Epsilon 0.5287081437599487\n",
      "Episode 25 Average Reward 29.64 Best Reward 70.0 Last Reward 41.0 Epsilon 0.5074590676632879\n",
      "Episode 26 Average Reward 30.076923076923077 Best Reward 70.0 Last Reward 41.0 Epsilon 0.48706400382327686\n",
      "Episode 27 Average Reward 30.85185185185185 Best Reward 70.0 Last Reward 51.0 Epsilon 0.46283472370715234\n",
      "Episode 28 Average Reward 30.464285714285715 Best Reward 70.0 Last Reward 20.0 Epsilon 0.4536654424342049\n",
      "Episode 29 Average Reward 29.896551724137932 Best Reward 70.0 Last Reward 14.0 Epsilon 0.44735524511437874\n",
      "Episode 30 Average Reward 30.033333333333335 Best Reward 70.0 Last Reward 34.0 Epsilon 0.43239347672187917\n",
      "Episode 31 Average Reward 29.70967741935484 Best Reward 70.0 Last Reward 20.0 Epsilon 0.42382727110771445\n",
      "Episode 32 Average Reward 29.6875 Best Reward 70.0 Last Reward 29.0 Epsilon 0.41170681546900234\n",
      "Episode 33 Average Reward 30.363636363636363 Best Reward 70.0 Last Reward 52.0 Epsilon 0.3908349959789493\n",
      "Episode 34 Average Reward 29.970588235294116 Best Reward 70.0 Last Reward 17.0 Epsilon 0.3842436897667367\n",
      "Episode 35 Average Reward 31.0 Best Reward 70.0 Last Reward 66.0 Epsilon 0.35969049949820686\n",
      "Episode 36 Average Reward 32.083333333333336 Best Reward 70.0 Last Reward 70.0 Epsilon 0.335361453123493\n",
      "Episode 37 Average Reward 32.729729729729726 Best Reward 70.0 Last Reward 56.0 Epsilon 0.31708849407160733\n",
      "Episode 38 Average Reward 34.8421052631579 Best Reward 113.0 Last Reward 113.0 Epsilon 0.28319178633180314\n",
      "Episode 39 Average Reward 35.61538461538461 Best Reward 113.0 Last Reward 65.0 Epsilon 0.26536117873480936\n",
      "Episode 40 Average Reward 37.725 Best Reward 120.0 Last Reward 120.0 Epsilon 0.2353401222790632\n",
      "Episode 41 Average Reward 40.51219512195122 Best Reward 152.0 Last Reward 152.0 Epsilon 0.20213903362537258\n",
      "Episode 42 Average Reward 41.642857142857146 Best Reward 152.0 Last Reward 88.0 Epsilon 0.1851028685133813\n",
      "Episode 43 Average Reward 43.04651162790697 Best Reward 152.0 Last Reward 102.0 Epsilon 0.16714483007507247\n",
      "Episode 44 Average Reward 48.40909090909091 Best Reward 279.0 Last Reward 279.0 Epsilon 0.12643408301419587\n",
      "Episode 45 Average Reward 51.31111111111111 Best Reward 279.0 Last Reward 179.0 Epsilon 0.10570281555543207\n",
      "Episode 46 Average Reward 54.04347826086956 Best Reward 279.0 Last Reward 177.0 Epsilon 0.08854783998018396\n",
      "Episode 47 Average Reward 55.340425531914896 Best Reward 279.0 Last Reward 115.0 Epsilon 0.07892400538576702\n",
      "Episode 48 Average Reward 57.3125 Best Reward 279.0 Last Reward 150.0 Epsilon 0.06792542297010071\n",
      "Episode 49 Average Reward 59.10204081632653 Best Reward 279.0 Last Reward 145.0 Epsilon 0.05875274256781066\n"
     ]
    }
   ],
   "source": [
    "agent = DQN_Solver()\n",
    "agent.network.train()\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "for i in range(1, EPISODES):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = agent.choose_action(state)\n",
    "        try:\n",
    "            q_values = action[1]\n",
    "            action = action[0]\n",
    "        except:\n",
    "            action = action\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        gradient_sign = agent.learn()\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score\n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "            break\n",
    "\n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734e8fb",
   "metadata": {},
   "source": [
    "## Testing the network with FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4560186e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Episode 1 Average Reward 94.0 Best Reward 94.0 Last Reward 94.0 Epsilon 0.05875274256781066\n",
      "41\n",
      "Episode 2 Average Reward 111.0 Best Reward 128.0 Last Reward 128.0 Epsilon 0.05875274256781066\n",
      "40\n",
      "Episode 3 Average Reward 116.0 Best Reward 128.0 Last Reward 126.0 Epsilon 0.05875274256781066\n",
      "56\n",
      "Episode 4 Average Reward 131.75 Best Reward 179.0 Last Reward 179.0 Epsilon 0.05875274256781066\n",
      "51\n",
      "Episode 5 Average Reward 140.0 Best Reward 179.0 Last Reward 173.0 Epsilon 0.05875274256781066\n",
      "57\n",
      "Episode 6 Average Reward 145.66666666666666 Best Reward 179.0 Last Reward 174.0 Epsilon 0.05875274256781066\n",
      "39\n",
      "Episode 7 Average Reward 142.71428571428572 Best Reward 179.0 Last Reward 125.0 Epsilon 0.05875274256781066\n",
      "38\n",
      "Episode 8 Average Reward 140.375 Best Reward 179.0 Last Reward 124.0 Epsilon 0.05875274256781066\n",
      "50\n",
      "Episode 9 Average Reward 143.55555555555554 Best Reward 179.0 Last Reward 169.0 Epsilon 0.05875274256781066\n",
      "72\n",
      "Episode 10 Average Reward 152.6 Best Reward 234.0 Last Reward 234.0 Epsilon 0.05875274256781066\n",
      "51\n",
      "Episode 11 Average Reward 153.54545454545453 Best Reward 234.0 Last Reward 163.0 Epsilon 0.05875274256781066\n",
      "64\n",
      "Episode 12 Average Reward 158.25 Best Reward 234.0 Last Reward 210.0 Epsilon 0.05875274256781066\n",
      "39\n",
      "Episode 13 Average Reward 155.92307692307693 Best Reward 234.0 Last Reward 128.0 Epsilon 0.05875274256781066\n",
      "48\n",
      "Episode 14 Average Reward 155.07142857142858 Best Reward 234.0 Last Reward 144.0 Epsilon 0.05875274256781066\n",
      "35\n",
      "Episode 15 Average Reward 152.06666666666666 Best Reward 234.0 Last Reward 110.0 Epsilon 0.05875274256781066\n",
      "51\n",
      "Episode 16 Average Reward 152.9375 Best Reward 234.0 Last Reward 166.0 Epsilon 0.05875274256781066\n",
      "47\n",
      "Episode 17 Average Reward 152.58823529411765 Best Reward 234.0 Last Reward 147.0 Epsilon 0.05875274256781066\n",
      "51\n",
      "Episode 18 Average Reward 152.94444444444446 Best Reward 234.0 Last Reward 159.0 Epsilon 0.05875274256781066\n",
      "46\n",
      "Episode 19 Average Reward 152.52631578947367 Best Reward 234.0 Last Reward 145.0 Epsilon 0.05875274256781066\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     24\u001b[0m     action \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m---> 25\u001b[0m state_, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(found_q_values \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     28\u001b[0m     next_q_values \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mnetwork(state_\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:139\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    132\u001b[0m     theta \u001b[38;5;241m=\u001b[39m theta \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m*\u001b[39m theta_dot\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mstack([x, x_dot, theta, theta_dot])\n\u001b[1;32m    136\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\n\u001b[1;32m    137\u001b[0m     x \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_threshold\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m x \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_threshold\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtheta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta_threshold_radians\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m theta \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta_threshold_radians\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    144\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Taking the gradient of the reward with respect to the temporal difference error\n",
    "agent.network.eval()\n",
    "\n",
    "gradient_sign = None\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "for i in range(1,100):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    did_pertubate = 0\n",
    "    while True:\n",
    "        found_q_values = False\n",
    "        env.render()\n",
    "        action = agent.choose_action(state)\n",
    "        try:\n",
    "            q_values = action[1]\n",
    "            action = action[0]\n",
    "            found_q_values = True\n",
    "        except Exception as e:\n",
    "            action = action\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        \n",
    "        if(found_q_values == True):\n",
    "            next_q_values = agent.network(state_.float())\n",
    "            loss = agent.network.loss(q_values, next_q_values)\n",
    "            \n",
    "            gradient_sign = torch.autograd.grad(loss,state,retain_graph=True)[0].sign()\n",
    "            if gradient_sign != None and score % 3 == 0:\n",
    "                state_ = state_ + gradient_sign * EPSILON_PERT\n",
    "                did_pertubate += 1\n",
    "                \n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            print(did_pertubate)\n",
    "            \n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score\n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "            break\n",
    "\n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946642dc",
   "metadata": {},
   "source": [
    "## Training the network with adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466343c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_robust = DQN_Solver()\n",
    "agent_robust.network.train()\n",
    "\n",
    "gradient_sign = None\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "for i in range(1, EPISODES):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    while True:\n",
    "        found_q_values = False\n",
    "        env.render()\n",
    "        action = agent_robust.choose_action(state)\n",
    "        try:\n",
    "            q_values = action[1]\n",
    "            action = action[0]\n",
    "            found_q_values = True\n",
    "        except Exception as e:\n",
    "            action = action\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        \n",
    "        if(found_q_values == True):\n",
    "            next_q_values = agent_robust.network(state_.float())\n",
    "            loss = agent_robust.network.loss(q_values, next_q_values)\n",
    "            \n",
    "            gradient_sign = torch.autograd.grad(loss,state,retain_graph=True)[0].sign()\n",
    "            if(gradient_sign != None):\n",
    "                state_ = state_ + gradient_sign * EPSILON_PERT\n",
    "            else:\n",
    "                print(\"Not perturbated\")\n",
    "                \n",
    "        agent_robust.memory.add(state, action, reward, state_, done)\n",
    "        agent_robust.learn()\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score\n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "            break\n",
    "\n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae29af",
   "metadata": {},
   "source": [
    "## Testing the network with FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e21a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the gradient of the reward with respect to the temporal difference error\n",
    "agent_robust.network.eval()\n",
    "\n",
    "gradient_sign = None\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "for i in range(1,100):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    while True:\n",
    "        found_q_values = False\n",
    "        env.render()\n",
    "        action = agent_robust.choose_action(state)\n",
    "        try:\n",
    "            q_values = action[1]\n",
    "            action = action[0]\n",
    "#             found_q_values = True\n",
    "        except Exception as e:\n",
    "            action = action\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        \n",
    "        if(found_q_values == True):\n",
    "            next_q_values = agent_robust.network(state_.float())\n",
    "            loss = agent_robust.network.loss(q_values, next_q_values)\n",
    "            \n",
    "            gradient_sign = torch.autograd.grad(loss,state,retain_graph=True)[0].sign()\n",
    "            if(gradient_sign != None):\n",
    "                state_ = state_ + gradient_sign * EPSILON_PERT\n",
    "            else:\n",
    "                print(\"Not perturbated\")\n",
    "                \n",
    "            \n",
    "        state = state_\n",
    "        score += reward\n",
    "        \n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score\n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "            break\n",
    "\n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338524a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
